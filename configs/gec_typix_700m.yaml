#########################################################
# GEC with moogin/typix-grammar-small (1.2B Gemma-based)
#########################################################

seed: 42

# Language Model
model_name: moogin/Typix-medium
max_seq_length: 512
system_prompt: "You are a writing assistant that edits text. Follow the user's instruction exactly. Preserve the original meaning unless the user asks to change it. Output only the revised text."

# Dataset (HuggingFace)
dataset_name: moogin/typix-hq-grannar
dataset_size: 10000  # 10k stratified sample

# Training hyperparameters
learning_rate: 5.0e-6
warmup_steps: 200
max_steps: 7500  # ~3 epochs (10k/4 * 3)
gradient_accumulation_steps: 8  # effective batch = 32

# vLLM inference
per_device_train_batch_size: 4
num_generations: 8
generation_batch_size: 8
max_completion_length: 256
use_vllm: true
vllm_mode: "colocate"
vllm_gpu_memory_utilization: 0.3

# GRPO specific
beta: 0.04  # KL penalty
temperature: 1.0  # Sampling temperature for diverse generations

# Reward weights
greco_weight: 0.6
semantic_weight: 0.3
laziness_weight: 0.1
gain_epsilon: 0.02

# Reward models (HuggingFace)
greco_model_name: mrqorib/grammaticality
mpnet_model: sentence-transformers/all-mpnet-base-v2

# LoRA configuration
use_peft: true
lora_r: 32
lora_alpha: 64
lora_dropout: 0.05
lora_bias: "none"
use_rslora: false
lora_target_modules:
  - "q_proj"
  - "k_proj"
  - "v_proj"
  - "o_proj"
  - "gate_proj"
  - "up_proj"
  - "down_proj"

# Experiment tracking
wandb_enabled: true
wandb_project_name: "gec-fine-tune-with-grpo"
logging_steps: 10
push_to_hf: false
