#########################################################
# GEC with moogin/typix-grammar-small (1.2B Gemma-based)
#########################################################

seed: 42

# Language Model
model_name: moogin/typix-grammar-small
max_seq_length: 512
system_prompt: |
  You are a grammatical error correction assistant.
  Correct any spelling and grammatical errors in the given text.
  Make minimal changes. Preserve the original meaning.
  Output only the corrected text, nothing else.

# Dataset (HuggingFace)
dataset_name: moogin/typix-hq-grannar
dataset_size: 10000  # 10k stratified sample

# Training hyperparameters
learning_rate: 5.0e-6
warmup_steps: 200
max_steps: 7500  # ~3 epochs (10k/4 * 3)
gradient_accumulation_steps: 8  # effective batch = 32

# vLLM inference
per_device_train_batch_size: 4
num_generations: 12
generation_batch_size: 12
max_completion_length: 256
use_vllm: true
vllm_mode: "colocate"
vllm_gpu_memory_utilization: 0.3

# GRPO specific
beta: 0.04  # KL penalty
temperature: 1.0  # Sampling temperature for diverse generations

# Reward weights
greco_weight: 0.6
semantic_weight: 0.3
laziness_weight: 0.1

# Reward models (HuggingFace)
greco_model_name: mrqorib/grammaticality
mpnet_model: sentence-transformers/all-mpnet-base-v2

# LoRA configuration
use_peft: true
lora_r: 32
lora_alpha: 64
lora_dropout: 0.05
lora_bias: "none"
use_rslora: false
lora_target_modules: "all-linear"

# Experiment tracking
wandb_enabled: true
wandb_project_name: "gec-fine-tune-with-grpo"
logging_steps: 10
push_to_hf: false
